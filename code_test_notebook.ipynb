{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_norm1 = nn.BatchNorm2d(num_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prelu1 = nn.PReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_norm2 = nn.BatchNorm2d(num_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,kernel_size,channels,padding,stride):\n",
    "        super(ResBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels,out_channels=channels,kernel_size=kernel_size,padding=padding,stride=stride)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=channels)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels,out_channels=channels,kernel_size=kernel_size,padding=padding,stride=stride)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=channels)\n",
    "    def forward(self,inp):\n",
    "        x = self.conv1(inp)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.prelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        output = x + inp\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resblock = ResBlock(3,64,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1,64,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resblock(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubPixelConv(nn.Module):\n",
    "    def __init__(self,kernel_size,in_channels,out_channels,padding,stride):\n",
    "        super(SubPixelConv,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,padding=padding,stride=stride)\n",
    "        self.pixel_shuffler1 = nn.PixelShuffle(2)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "    def forward(self,inp):\n",
    "        x = self.conv1(inp)\n",
    "        x = self.pixel_shuffler1(x)\n",
    "        output = self.prelu1(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 4, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subpixelconv(resblock(inp)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubPixelConv,self).__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module(module=nn.Conv2d(in_channels=3,out_channels=64,kernel_size=9,padding=4,stride=1),name='conv1')\n",
    "        self.model.add_module(module=nn.PReLU(),name='prelu1')\n",
    "        for i in range(16):\n",
    "            self.model.add_module(module=ResBlock(3,64,1,1),name='res'+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "  (prelu1): PReLU(num_parameters=1)\n",
       "  (res1): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res2): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res3): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res4): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res5): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res6): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res7): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res8): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res9): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res10): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res11): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res12): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res13): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res14): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res15): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res16): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu1): PReLU(num_parameters=1)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 96, 96]          15,616\n",
      "             PReLU-2           [-1, 64, 96, 96]               1\n",
      "            Conv2d-3           [-1, 64, 96, 96]          36,928\n",
      "       BatchNorm2d-4           [-1, 64, 96, 96]             128\n",
      "             PReLU-5           [-1, 64, 96, 96]               1\n",
      "            Conv2d-6           [-1, 64, 96, 96]          36,928\n",
      "       BatchNorm2d-7           [-1, 64, 96, 96]             128\n",
      "          ResBlock-8           [-1, 64, 96, 96]               0\n",
      "            Conv2d-9           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-10           [-1, 64, 96, 96]             128\n",
      "            PReLU-11           [-1, 64, 96, 96]               1\n",
      "           Conv2d-12           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-13           [-1, 64, 96, 96]             128\n",
      "         ResBlock-14           [-1, 64, 96, 96]               0\n",
      "           Conv2d-15           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-16           [-1, 64, 96, 96]             128\n",
      "            PReLU-17           [-1, 64, 96, 96]               1\n",
      "           Conv2d-18           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-19           [-1, 64, 96, 96]             128\n",
      "         ResBlock-20           [-1, 64, 96, 96]               0\n",
      "           Conv2d-21           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-22           [-1, 64, 96, 96]             128\n",
      "            PReLU-23           [-1, 64, 96, 96]               1\n",
      "           Conv2d-24           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-25           [-1, 64, 96, 96]             128\n",
      "         ResBlock-26           [-1, 64, 96, 96]               0\n",
      "           Conv2d-27           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-28           [-1, 64, 96, 96]             128\n",
      "            PReLU-29           [-1, 64, 96, 96]               1\n",
      "           Conv2d-30           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-31           [-1, 64, 96, 96]             128\n",
      "         ResBlock-32           [-1, 64, 96, 96]               0\n",
      "           Conv2d-33           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-34           [-1, 64, 96, 96]             128\n",
      "            PReLU-35           [-1, 64, 96, 96]               1\n",
      "           Conv2d-36           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-37           [-1, 64, 96, 96]             128\n",
      "         ResBlock-38           [-1, 64, 96, 96]               0\n",
      "           Conv2d-39           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-40           [-1, 64, 96, 96]             128\n",
      "            PReLU-41           [-1, 64, 96, 96]               1\n",
      "           Conv2d-42           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-43           [-1, 64, 96, 96]             128\n",
      "         ResBlock-44           [-1, 64, 96, 96]               0\n",
      "           Conv2d-45           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-46           [-1, 64, 96, 96]             128\n",
      "            PReLU-47           [-1, 64, 96, 96]               1\n",
      "           Conv2d-48           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-49           [-1, 64, 96, 96]             128\n",
      "         ResBlock-50           [-1, 64, 96, 96]               0\n",
      "           Conv2d-51           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-52           [-1, 64, 96, 96]             128\n",
      "            PReLU-53           [-1, 64, 96, 96]               1\n",
      "           Conv2d-54           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-55           [-1, 64, 96, 96]             128\n",
      "         ResBlock-56           [-1, 64, 96, 96]               0\n",
      "           Conv2d-57           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-58           [-1, 64, 96, 96]             128\n",
      "            PReLU-59           [-1, 64, 96, 96]               1\n",
      "           Conv2d-60           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-61           [-1, 64, 96, 96]             128\n",
      "         ResBlock-62           [-1, 64, 96, 96]               0\n",
      "           Conv2d-63           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-64           [-1, 64, 96, 96]             128\n",
      "            PReLU-65           [-1, 64, 96, 96]               1\n",
      "           Conv2d-66           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-67           [-1, 64, 96, 96]             128\n",
      "         ResBlock-68           [-1, 64, 96, 96]               0\n",
      "           Conv2d-69           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-70           [-1, 64, 96, 96]             128\n",
      "            PReLU-71           [-1, 64, 96, 96]               1\n",
      "           Conv2d-72           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-73           [-1, 64, 96, 96]             128\n",
      "         ResBlock-74           [-1, 64, 96, 96]               0\n",
      "           Conv2d-75           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-76           [-1, 64, 96, 96]             128\n",
      "            PReLU-77           [-1, 64, 96, 96]               1\n",
      "           Conv2d-78           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-79           [-1, 64, 96, 96]             128\n",
      "         ResBlock-80           [-1, 64, 96, 96]               0\n",
      "           Conv2d-81           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-82           [-1, 64, 96, 96]             128\n",
      "            PReLU-83           [-1, 64, 96, 96]               1\n",
      "           Conv2d-84           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-85           [-1, 64, 96, 96]             128\n",
      "         ResBlock-86           [-1, 64, 96, 96]               0\n",
      "           Conv2d-87           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-88           [-1, 64, 96, 96]             128\n",
      "            PReLU-89           [-1, 64, 96, 96]               1\n",
      "           Conv2d-90           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-91           [-1, 64, 96, 96]             128\n",
      "         ResBlock-92           [-1, 64, 96, 96]               0\n",
      "           Conv2d-93           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-94           [-1, 64, 96, 96]             128\n",
      "            PReLU-95           [-1, 64, 96, 96]               1\n",
      "           Conv2d-96           [-1, 64, 96, 96]          36,928\n",
      "      BatchNorm2d-97           [-1, 64, 96, 96]             128\n",
      "         ResBlock-98           [-1, 64, 96, 96]               0\n",
      "================================================================\n",
      "Total params: 1,201,425\n",
      "Trainable params: 1,201,425\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 441.00\n",
      "Params size (MB): 4.58\n",
      "Estimated Total Size (MB): 445.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(generator.model,input_size=(3,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.3964, -0.2138],\n",
       "          [ 0.7101, -1.4748]],\n",
       "\n",
       "         [[-0.6391,  1.4618],\n",
       "          [ 1.2519,  0.0663]],\n",
       "\n",
       "         [[-0.3244, -0.7537],\n",
       "          [-1.5978, -0.4525]],\n",
       "\n",
       "         [[ 0.4607,  0.1563],\n",
       "          [ 1.0554, -2.6950]],\n",
       "\n",
       "         [[-0.9548, -0.1770],\n",
       "          [ 0.2713, -0.7973]],\n",
       "\n",
       "         [[ 2.3619,  0.3170],\n",
       "          [-0.9167, -0.2072]],\n",
       "\n",
       "         [[ 0.0494, -2.0885],\n",
       "          [ 0.3592, -0.2938]],\n",
       "\n",
       "         [[ 0.5089,  0.4162],\n",
       "          [ 0.2064, -1.8366]],\n",
       "\n",
       "         [[-0.4067, -0.0246],\n",
       "          [-1.3946, -0.6146]],\n",
       "\n",
       "         [[ 0.3186, -1.3618],\n",
       "          [ 2.6360,  0.5912]],\n",
       "\n",
       "         [[-2.0839, -0.0138],\n",
       "          [ 2.6350,  0.1775]],\n",
       "\n",
       "         [[ 1.3910, -0.7048],\n",
       "          [ 0.1877, -1.1083]],\n",
       "\n",
       "         [[ 0.6507,  0.3739],\n",
       "          [ 0.0545,  0.2233]],\n",
       "\n",
       "         [[-0.1060,  2.7515],\n",
       "          [ 0.7918, -0.2241]],\n",
       "\n",
       "         [[-0.3089, -1.1391],\n",
       "          [-0.0386,  0.6891]],\n",
       "\n",
       "         [[-1.0344, -1.2680],\n",
       "          [-0.0309, -1.0123]],\n",
       "\n",
       "         [[-0.9440, -0.6122],\n",
       "          [ 1.1287, -0.2686]],\n",
       "\n",
       "         [[-1.5150,  0.9407],\n",
       "          [ 0.3561, -2.1801]],\n",
       "\n",
       "         [[ 0.3339, -1.1106],\n",
       "          [-0.6192, -0.6454]],\n",
       "\n",
       "         [[ 3.0936, -1.6695],\n",
       "          [-0.5603,  1.4323]],\n",
       "\n",
       "         [[ 0.2730,  0.4412],\n",
       "          [-0.1860,  0.4409]],\n",
       "\n",
       "         [[ 0.4430,  2.0447],\n",
       "          [ 1.2554, -1.5997]],\n",
       "\n",
       "         [[-0.2342,  2.0258],\n",
       "          [-0.3655, -0.3033]],\n",
       "\n",
       "         [[ 1.2537,  0.2387],\n",
       "          [-2.1440, -0.3032]],\n",
       "\n",
       "         [[-0.7001, -0.1876],\n",
       "          [ 2.0129,  0.3531]],\n",
       "\n",
       "         [[ 0.3915, -0.5188],\n",
       "          [-0.7818,  1.6643]],\n",
       "\n",
       "         [[-1.9290,  1.6296],\n",
       "          [-1.7139, -2.1792]],\n",
       "\n",
       "         [[-1.0001, -2.2304],\n",
       "          [ 0.4672,  1.3830]],\n",
       "\n",
       "         [[ 0.4404,  0.0468],\n",
       "          [ 0.4957, -0.7920]],\n",
       "\n",
       "         [[-0.2861,  0.2305],\n",
       "          [ 0.5584, -0.0804]],\n",
       "\n",
       "         [[-0.5189, -0.6955],\n",
       "          [ 0.5508,  1.1611]],\n",
       "\n",
       "         [[-0.5846, -0.6453],\n",
       "          [-0.7524, -0.1305]],\n",
       "\n",
       "         [[ 0.7029, -0.8880],\n",
       "          [ 3.4578,  0.7509]],\n",
       "\n",
       "         [[-0.1337,  3.1518],\n",
       "          [-1.6848, -0.5695]],\n",
       "\n",
       "         [[-2.4364, -0.3714],\n",
       "          [ 0.9097,  1.9480]],\n",
       "\n",
       "         [[-1.5934,  2.2500],\n",
       "          [-1.9727, -0.3129]],\n",
       "\n",
       "         [[-0.9207,  0.3352],\n",
       "          [-0.0511, -1.6329]],\n",
       "\n",
       "         [[ 0.1493,  2.4518],\n",
       "          [-1.8006, -1.4772]],\n",
       "\n",
       "         [[ 0.6610,  0.0160],\n",
       "          [-0.9190,  1.1576]],\n",
       "\n",
       "         [[ 1.6076, -0.6921],\n",
       "          [ 2.2440, -0.2642]],\n",
       "\n",
       "         [[-0.3983, -0.7152],\n",
       "          [ 0.2397,  3.0732]],\n",
       "\n",
       "         [[ 1.3499,  2.2826],\n",
       "          [-1.8340, -1.2462]],\n",
       "\n",
       "         [[ 1.2005,  0.1763],\n",
       "          [ 0.6601, -0.3636]],\n",
       "\n",
       "         [[ 0.8530,  0.4566],\n",
       "          [ 2.0650, -0.6923]],\n",
       "\n",
       "         [[ 1.5297, -2.2577],\n",
       "          [ 0.0260, -0.4099]],\n",
       "\n",
       "         [[-0.8410,  2.7492],\n",
       "          [-0.6760, -0.1446]],\n",
       "\n",
       "         [[ 0.2845,  0.4078],\n",
       "          [-2.3014, -0.5616]],\n",
       "\n",
       "         [[ 1.1583, -0.9878],\n",
       "          [ 0.0115,  2.0391]],\n",
       "\n",
       "         [[-1.2508, -2.1567],\n",
       "          [ 1.7397, -0.0069]],\n",
       "\n",
       "         [[ 1.5652,  1.9672],\n",
       "          [ 0.2520, -0.7496]],\n",
       "\n",
       "         [[ 1.1467, -1.1261],\n",
       "          [ 0.6652, -0.2190]],\n",
       "\n",
       "         [[-1.0174, -1.4787],\n",
       "          [-0.4483,  0.3599]],\n",
       "\n",
       "         [[ 1.1997,  0.3852],\n",
       "          [-0.9634, -1.3169]],\n",
       "\n",
       "         [[-1.0181,  0.5507],\n",
       "          [ 1.4600,  0.6746]],\n",
       "\n",
       "         [[ 1.4516, -0.8542],\n",
       "          [ 0.2466,  2.3294]],\n",
       "\n",
       "         [[-0.1220,  0.3419],\n",
       "          [ 0.9478, -0.7999]],\n",
       "\n",
       "         [[-0.9216,  1.7861],\n",
       "          [-0.5819, -1.3721]],\n",
       "\n",
       "         [[ 1.0763,  1.0959],\n",
       "          [-1.2825,  0.5779]],\n",
       "\n",
       "         [[ 1.1726,  0.9044],\n",
       "          [-1.0172, -0.2079]],\n",
       "\n",
       "         [[-0.0693,  2.6835],\n",
       "          [ 0.5565, -0.0281]],\n",
       "\n",
       "         [[ 0.4890, -0.7141],\n",
       "          [-0.7176,  0.2664]],\n",
       "\n",
       "         [[-0.1504, -1.7921],\n",
       "          [-1.1899,  1.4457]],\n",
       "\n",
       "         [[-0.0369,  0.9044],\n",
       "          [-0.8740,  1.1933]],\n",
       "\n",
       "         [[ 0.6252, -1.3200],\n",
       "          [ 0.8159, -2.1984]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "resblock = list(model.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resblock.forward(inp) - model.forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(resblock.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inp\n",
    "for layer in layers:\n",
    "    x = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.]]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+inp - resblock.forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
